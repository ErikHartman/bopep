{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/ErikHartman/bopep/blob/main/bopep.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X2L-Q7cEeUXx"
   },
   "source": [
    "# BoPep: identifying peptide binders in large scale peptidomic data\n",
    "\n",
    "Bayesian optimization guided search for binders in large scale peptidomic datasets.\n",
    "\n",
    "Relies on ESM2 for peptide embeddings, ColabFold utilizing AlphaFold 2 multimer v3 for docking, and PyRosetta for interface quality calculations. A deep ensemble is used as a surrogate model utilizing Torch. \n",
    "\n",
    "Note that the use of PyRosetta for **commercial** purposes requires purchasing a license.\n",
    "\n",
    "Set runtime to T4 GPU (Runtime > Change runtime type)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wX-T-BaXe8NS",
    "outputId": "0e3519af-12cd-4714-fcf8-023f0125e119"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching bopep\n",
      "Cloning into '/content/bopep'...\n",
      "remote: Enumerating objects: 96, done.\u001b[K\n",
      "remote: Counting objects: 100% (96/96), done.\u001b[K\n",
      "remote: Compressing objects: 100% (79/79), done.\u001b[K\n",
      "remote: Total 96 (delta 42), reused 57 (delta 15), pack-reused 0 (from 0)\u001b[K\n",
      "Receiving objects: 100% (96/96), 107.15 KiB | 5.64 MiB/s, done.\n",
      "Resolving deltas: 100% (42/42), done.\n",
      "Installing necessary packages using pip\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m56.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.5/19.5 MB\u001b[0m \u001b[31m82.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m51.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.2/41.2 MB\u001b[0m \u001b[31m44.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m797.1/797.1 MB\u001b[0m \u001b[31m36.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m56.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m136.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m104.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m37.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m29.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m62.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m61.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m62.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m69.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.2/176.2 MB\u001b[0m \u001b[31m55.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.4/209.4 MB\u001b[0m \u001b[31m69.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "cudf-cu12 24.10.1 requires pandas<2.2.3dev0,>=2.0, but you have pandas 2.2.3 which is incompatible.\n",
      "cupy-cuda12x 12.2.0 requires numpy<1.27,>=1.20, but you have numpy 2.0.2 which is incompatible.\n",
      "gensim 4.3.3 requires numpy<2.0,>=1.18.5, but you have numpy 2.0.2 which is incompatible.\n",
      "gensim 4.3.3 requires scipy<1.14.0,>=1.7.0, but you have scipy 1.14.1 which is incompatible.\n",
      "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\n",
      "langchain 0.3.7 requires numpy<2,>=1; python_version < \"3.12\", but you have numpy 2.0.2 which is incompatible.\n",
      "matplotlib 3.8.0 requires numpy<2,>=1.21, but you have numpy 2.0.2 which is incompatible.\n",
      "pytensor 2.25.5 requires numpy<2,>=1.17.0, but you have numpy 2.0.2 which is incompatible.\n",
      "tensorflow 2.17.0 requires numpy<2.0.0,>=1.23.5; python_version <= \"3.11\", but you have numpy 2.0.2 which is incompatible.\n",
      "thinc 8.2.5 requires numpy<2.0.0,>=1.19.0; python_version >= \"3.9\", but you have numpy 2.0.2 which is incompatible.\n",
      "torchaudio 2.5.0+cu121 requires torch==2.5.0, but you have torch 2.4.1 which is incompatible.\n",
      "torchvision 0.20.0+cu121 requires torch==2.5.0, but you have torch 2.4.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mInstalling ColabFold\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m93.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m163.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.1/12.1 MB\u001b[0m \u001b[31m145.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m230.0/230.0 MB\u001b[0m \u001b[31m50.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m64.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m106.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "cudf-cu12 24.10.1 requires pandas<2.2.3dev0,>=2.0, but you have pandas 1.5.3 which is incompatible.\n",
      "gensim 4.3.3 requires scipy<1.14.0,>=1.7.0, but you have scipy 1.14.1 which is incompatible.\n",
      "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 1.5.3 which is incompatible.\n",
      "mizani 0.13.0 requires pandas>=2.2.0, but you have pandas 1.5.3 which is incompatible.\n",
      "opentelemetry-api 1.28.0 requires importlib-metadata<=8.5.0,>=6.0, but you have importlib-metadata 4.13.0 which is incompatible.\n",
      "plotnine 0.14.1 requires pandas>=2.2.0, but you have pandas 1.5.3 which is incompatible.\n",
      "tensorflow 2.17.0 requires tensorboard<2.18,>=2.17, but you have tensorboard 2.18.0 which is incompatible.\n",
      "torchvision 0.20.0+cu121 requires torch==2.5.0, but you have torch 2.4.1 which is incompatible.\n",
      "xarray 2024.10.0 requires pandas>=2.1, but you have pandas 1.5.3 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mInstalling fair-esm\n",
      "Downloading ESM model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://dl.fbaipublicfiles.com/fair-esm/models/esm2_t33_650M_UR50D.pt\" to /root/.cache/torch/hub/checkpoints/esm2_t33_650M_UR50D.pt\n",
      "Downloading: \"https://dl.fbaipublicfiles.com/fair-esm/regression/esm2_t33_650M_UR50D-contact-regression.pt\" to /root/.cache/torch/hub/checkpoints/esm2_t33_650M_UR50D-contact-regression.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing PyRosetta\n",
      "Collecting pyrosettacolabsetup\n",
      "  Downloading pyrosettacolabsetup-1.0.9-py3-none-any.whl.metadata (294 bytes)\n",
      "Downloading pyrosettacolabsetup-1.0.9-py3-none-any.whl (4.9 kB)\n",
      "Installing collected packages: pyrosettacolabsetup\n",
      "Successfully installed pyrosettacolabsetup-1.0.9\n",
      "\n",
      "Note that USE OF PyRosetta FOR COMMERCIAL PURPOSES REQUIRE PURCHASE OF A LICENSE.\n",
      "See https://github.com/RosettaCommons/rosetta/blob/main/LICENSE.md or email license@uw.edu for details.\n",
      "\n",
      "Looking for compatible PyRosetta wheel file at google-drive/PyRosetta/colab.bin//wheels.serialization...\n",
      "Downloading PyRosetta package...\n",
      "\n",
      "Resolving west.rosettacommons.org (west.rosettacommons.org)... 128.95.160.153, 2607:4000:406::160:153\n",
      "\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://west.rosettacommons.org/pyrosetta/release/release/PyRosetta4.Release.python310.ubuntu.cxx11thread.serialization.wheel/pyrosetta-2024.42+release.3366cf78a3-cp310-cp310-linux_x86_64.whl [following]\n",
      "--2024-11-13 08:31:49--  https://west.rosettacommons.org/pyrosetta/release/release/PyRosetta4.Release.python310.ubuntu.cxx11thread.serialization.wheel/pyrosetta-2024.42+release.3366cf78a3-cp310-cp310-linux_x86_64.whl\n",
      "\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1576918002 (1.5G)\n",
      "--2024-11-13 08:31:49--  https://west.rosettacommons.org/pyrosetta/release/release/PyRosetta4.Release.python310.ubuntu.cxx11thread.serialization.wheel/pyrosetta-2024.42+release.3366cf78a3-cp310-cp310-linux_x86_64.whl\n",
      "Reusing existing connection to west.rosettacommons.org:443.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1576918002 (1.5G)\n",
      "Saving to: ‘/PyRosetta/colab.bin/tmpad7_bdid/pyrosetta-2024.42+release.3366cf78a3-cp310-cp310-linux_x86_64.whl’\n",
      "\n",
      "pyrosetta-2024.42+r 100%[===================>]   1.47G  10.6MB/s    in 2m 53s  \n",
      "\n",
      "2024-11-13 08:34:43 (8.68 MB/s) - ‘/PyRosetta/colab.bin/tmpad7_bdid/pyrosetta-2024.42+release.3366cf78a3-cp310-cp310-linux_x86_64.whl’ saved [1576918002/1576918002]\n",
      "Moving wheel file pyrosetta-2024.42+release.3366cf78a3-cp310-cp310-linux_x86_64.whl to target dir /PyRosetta/colab.bin/wheels.serialization...\n",
      "Installing PyRosetta wheel '/PyRosetta/colab.bin/wheels.serialization/pyrosetta-2024.42+release.3366cf78a3-cp310-cp310-linux_x86_64.whl'...\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#@title Installation\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "\n",
    "# Set cache directory to /content to ensure we have write permissions\n",
    "os.environ['XDG_CACHE_HOME'] = '/content'\n",
    "\n",
    "# Check Python version\n",
    "PYTHON_VERSION = f\"{sys.version_info.major}.{sys.version_info.minor}\"\n",
    "print(f\"Using Python version: {PYTHON_VERSION}\")\n",
    "\n",
    "# Install ColabFold\n",
    "try:\n",
    "    import colabfold\n",
    "    print(\"ColabFold is already installed.\")\n",
    "except ImportError:\n",
    "    print(\"Installing ColabFold...\")\n",
    "    !pip install -q --no-warn-conflicts 'colabfold[alphafold-minus-jax] @ git+https://github.com/sokrypton/ColabFold'\n",
    "\n",
    "# Install JAX with CUDA support\n",
    "try:\n",
    "    import jax\n",
    "    if jax.local_devices()[0].platform == 'gpu':\n",
    "        print(\"JAX is already installed with CUDA support.\")\n",
    "    else:\n",
    "        print(\"Reinstalling JAX with CUDA support...\")\n",
    "        !pip uninstall -y jax jaxlib\n",
    "        !pip install -q --no-warn-conflicts --upgrade dm-haiku==0.0.10 'jax[cuda12_pip]'==0.3.25 -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html\n",
    "except ImportError:\n",
    "    print(\"Installing JAX with CUDA support...\")\n",
    "    !pip uninstall -y jax jaxlib\n",
    "    !pip install -q --no-warn-conflicts --upgrade dm-haiku==0.0.10 'jax[cuda12_pip]'==0.3.25 -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html\n",
    "\n",
    "# Install HH-suite via apt-get\n",
    "if shutil.which(\"hhsearch\") is not None:\n",
    "    print(\"HH-suite is already installed.\")\n",
    "else:\n",
    "    print(\"Installing HH-suite...\")\n",
    "    !apt-get update\n",
    "    !apt-get install -y hhsuite\n",
    "\n",
    "# Install Kalign via apt-get (required for MSA generation)\n",
    "if shutil.which(\"kalign\") is not None:\n",
    "    print(\"Kalign is already installed.\")\n",
    "else:\n",
    "    print(\"Installing Kalign...\")\n",
    "    !apt-get install -y kalign\n",
    "\n",
    "# Install OpenMM and PDBFixer via pip\n",
    "try:\n",
    "    import openmm\n",
    "    import pdbfixer\n",
    "    print(\"OpenMM and PDBFixer are already installed.\")\n",
    "except ImportError:\n",
    "    print(\"Installing OpenMM and PDBFixer...\")\n",
    "    !pip install --quiet openmm\n",
    "    pdbfixer_repo_url = \"https://github.com/openmm/pdbfixer.git\"\n",
    "    pdbfixer_dir = \"/content/pdbfixer\"\n",
    "\n",
    "    if not os.path.exists(pdbfixer_dir):\n",
    "        print(\"Cloning the PDBFixer repository...\")\n",
    "        !git clone {pdbfixer_repo_url} {pdbfixer_dir}\n",
    "    else:\n",
    "        print(\"PDBFixer repository already cloned.\")\n",
    "\n",
    "    print(\"Installing PDBFixer...\")\n",
    "    !pip install {pdbfixer_dir}\n",
    "\n",
    "# Fetch bopep repository\n",
    "if not os.path.exists(\"/content/bopep\"):\n",
    "    print(\"Fetching bopep...\")\n",
    "    !git clone https://github.com/ErikHartman/bopep /content/bopep/\n",
    "    print(\"Installing necessary packages using pip...\")\n",
    "    !pip install -r /content/bopep/requirements.txt --quiet\n",
    "else:\n",
    "    print(\"bopep repository already exists.\")\n",
    "\n",
    "# Install ESM model\n",
    "esm_model_path = \"/root/.cache/torch/hub/checkpoints/esm2_t33_650M_UR50D.pt\"\n",
    "if not os.path.exists(esm_model_path):\n",
    "    print(\"Installing fair-esm...\")\n",
    "    !pip install --quiet fair-esm\n",
    "    print(\"Downloading ESM model...\")\n",
    "    import esm\n",
    "    model, alphabet = esm.pretrained.esm2_t33_650M_UR50D()\n",
    "else:\n",
    "    print(\"ESM model already exists.\")\n",
    "\n",
    "# Download AlphaFold model parameters\n",
    "params_dir = Path(\"/root/.cache/colabfold/\")\n",
    "expected_param_files = [\n",
    "    params_dir / f\"params_model_{model_num}_multimer_v3.npz\"\n",
    "    for model_num in range(1, 6)\n",
    "]\n",
    "if all(param_file.exists() for param_file in expected_param_files):\n",
    "    print(\"AlphaFold model parameters already downloaded.\")\n",
    "else:\n",
    "    !mkdir /root/.cache/colabfold/\n",
    "    !mkdir /root/.cache/colabfold/params\n",
    "    print(\"Downloading AlphaFold model parameters...\")\n",
    "    from colabfold.download import download_alphafold_params\n",
    "    download_alphafold_params(\"alphafold2_multimer_v3\", params_dir)\n",
    "    !scp /root/.cache/colabfold /content\n",
    "\n",
    "# Install PyRosetta\n",
    "try:\n",
    "    import pyrosetta\n",
    "    print(\"PyRosetta is already installed.\")\n",
    "except ImportError:\n",
    "    try:\n",
    "        import pyrosettacolabsetup\n",
    "    except ImportError:\n",
    "        print(\"Installing pyrosettacolabsetup...\")\n",
    "        !pip install pyrosettacolabsetup\n",
    "    print(\"Installing PyRosetta...\")\n",
    "    import pyrosettacolabsetup\n",
    "    pyrosettacolabsetup.install_pyrosetta(serialization=True, cache_wheel_on_google_drive=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X-sd12-jfCBl",
    "outputId": "fdcbe5dc-ea95-41f5-a660-384bce65a67e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/bopep\n"
     ]
    }
   ],
   "source": [
    "#@title Embedding Settings\n",
    "import pandas as pd\n",
    "\n",
    "%cd bopep\n",
    "\n",
    "# Data input\n",
    "# @markdown Upload your input data file and set the path.\n",
    "data_file = \"/content/bopep/data/test_data.csv\" #@param {type:\"string\"}\n",
    "\n",
    "if not os.path.exists(data_file):\n",
    "  raise ValueError(\"The data file does not exist in the path.\")\n",
    "\n",
    "data = pd.read_csv(data_file)  # Load the CSV file\n",
    "peptides = data[\"peptide\"].tolist()\n",
    "# @markdown  ### Filtering options:\n",
    "\n",
    "# @markdown Set maximum and minimum peptide length\n",
    "max_length = 30  #@param {type:\"slider\", min:10, max:60, step:1}\n",
    "min_length = 5   #@param {type:\"slider\", min:1, max:30, step:1}\n",
    "\n",
    "# @markdown Set maximum repeat length for amino acids\n",
    "max_repeat_length = 5  #@param {type:\"slider\", min:1, max:15, step:1}\n",
    "\n",
    "# @markdown  Set maximum allowed fraction of single amino acids\n",
    "max_single_aa_fraction = 0.73  #@param {type:\"slider\", min:0, max:1, step:0.01}\n",
    "\n",
    "# @markdown  Variance kept during PCA reduction\n",
    "pca_variance = 0.95  #@param {type:\"slider\", min:0.1, max:1, step:0.01}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "duvhwIxSh338",
    "outputId": "99266c63-5953-4a13-c165-917c6af86c5d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/esm/pretrained.py:70: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model_data = torch.load(str(model_location), map_location=\"cpu\")\n",
      "/usr/local/lib/python3.10/dist-packages/esm/pretrained.py:74: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  regression_data = torch.load(regression_location, map_location=\"cpu\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model moved to GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating embeddings: 100%|██████████| 16/16 [00:12<00:00,  1.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reduced embedding size: (1000, 96) (before PCA: (1000, 1280))\n"
     ]
    }
   ],
   "source": [
    "#@title Generate Embeddings\n",
    "from src.embeddings.embed import embed\n",
    "from src.embeddings.utils import filter_peptides\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "\n",
    "filtered_peptides = filter_peptides(peptides, max_single_aa_fraction, max_repeat_length, min_length, max_length)\n",
    "embeddings = embed(filtered_peptides, model_path=esm_model_path)\n",
    "\n",
    "if pca_variance < 1:\n",
    "  embedding_array = np.array(list(embeddings.values()))\n",
    "  peptide_sequences = list(embeddings.keys())\n",
    "  pca = PCA(n_components=0.95, svd_solver=\"full\")\n",
    "  embeddings_reduced = pca.fit_transform(embedding_array)\n",
    "  print(f\"Reduced embedding size: {np.shape(embeddings_reduced)} (before PCA: {np.shape(embedding_array)})\")\n",
    "  embeddings = {\n",
    "      peptide_sequences[i]: embeddings_reduced[i] for i in range(len(peptide_sequences))\n",
    "  }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zz07Koj-fIWi"
   },
   "outputs": [],
   "source": [
    "#@title Bayesian Optimization Settings\n",
    "\n",
    "from src.docking.dock_peptides import extract_sequence_from_pdb\n",
    "\n",
    "# Target structure file (PDB format)\n",
    "target_structure = \"/content/bopep/data/4glp.pdb\"  #@param {type:\"string\"}\n",
    "target_sequence = extract_sequence_from_pdb(target_structure, \"A\")\n",
    "\n",
    "# @markdown Model settings\n",
    "num_recycles = 9  #@param {type:\"slider\", min:1, max:20, step:1}  # Number of recycles for AlphaFold\n",
    "num_relax = 1  #@param {type:\"slider\", min:0, max:5, step:1}  # Number of relaxations\n",
    "num_models = 5  #@param {type:\"slider\", min:1, max:5, step:1}  # Number of models\n",
    "num_processes = 2  #@param {type:\"slider\", min:1, max:8, step:1}  # Number of CPU processes for docking\n",
    "gpu_ids = [\"0\"]  #@param {type:\"hidden\"}  # List of GPU IDs, Colab generally has one GPU\n",
    "relax_max_iterations = 200 #@param [0, 200, 2000] {type:\"raw\"}\n",
    "\n",
    "# @markdown Stopping and relaxation parameters\n",
    "recycle_early_stop_tolerance = 0.3  #@param {type:\"slider\", min:0, max:1, step:0.1}  # Early stop tolerance\n",
    "amber = True  #@param {type:\"boolean\"}  # Whether to use AMBER for relaxation\n",
    "\n",
    "# @markdown Target binding site (optional)\n",
    "binding_site_residue_indices = [44, 49, 74, 82, 89, 105]  #@param {type:\"raw\"}  # Binding site residues\n",
    "\n",
    "# @markdown Objective weights for Bayesian optimization\n",
    "iptm_score_weight = 0.5  #@param {type:\"number\"}\n",
    "interface_sasa_weight = 0.1  #@param {type:\"number\"}\n",
    "interface_dG_weight = 0.1  #@param {type:\"number\"}\n",
    "rosetta_score_weight = 0.1  #@param {type:\"number\"}\n",
    "interface_delta_hbond_unsat_weight = 0.1  #@param {type:\"number\"}\n",
    "packstat_weight = 0.1  #@param {type:\"number\"}\n",
    "\n",
    "# @markdown Bayesian Optimization Iterations\n",
    "n_initial = 30  #@param {type:\"slider\", min:50, max:1000, step:1}  # Initial number of evaluations\n",
    "n_exploration_iterations = 10  #@param {type:\"slider\", min:50, max:1500, step:1}  # Number of exploration iterations\n",
    "n_exploitation_with_distance_weight = 10  #@param {type:\"slider\", min:50, max:3000, step:50}  # Exploitation iterations with distance weight\n",
    "n_exploitation_iterations = 0  #@param {type:\"slider\", min:0, max:1000, step:1}  # Number of exploitation iterations without distance weight\n",
    "batch_size = 4  #@param {type:\"slider\", min:1, max:32, step:1}  # Batch size for optimization\n",
    "agreeing_models = 0  #@param {type:\"slider\", min:0, max:10, step:1}  # Number of agreeing models to use\n",
    "proximity_threshold = 5.0  #@param {type:\"slider\", min:1, max:20, step:0.5}  # Proximity threshold in Ångstroms\n",
    "hparam_opt_interval = 10  #@param {type:\"slider\", min:10, max:200, step:10}  # Hyperparameter optimization interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Hyperparameter Optimization Settings\n",
    "\n",
    "# @markdown Number of layers\n",
    "n_layers_min = 1  #@param {type:\"slider\", min:1, max:10, step:1}\n",
    "n_layers_max = 5  #@param {type:\"slider\", min:1, max:10, step:1}\n",
    "n_layers_range = (n_layers_min, n_layers_max)\n",
    "\n",
    "# @markdown Units in the first layer (log scale)\n",
    "n_units_l1_min = 32  #@param {type:\"slider\", min:32, max:4096, step:32}\n",
    "n_units_l1_max = 1024  #@param {type:\"slider\", min:32, max:4096, step:32}\n",
    "n_units_l1_range = (n_units_l1_min, n_units_l1_max)\n",
    "\n",
    "# @markdown Alpha range (log scale)\n",
    "alpha_min = 1e-5  #@param {type:\"number\"}\n",
    "alpha_max = 1e-3  #@param {type:\"number\"}\n",
    "alpha_range = (alpha_min, alpha_max)\n",
    "\n",
    "# @markdown Learning rate initialization range (log scale)\n",
    "learning_rate_init_min = 1e-4  #@param {type:\"number\"}\n",
    "learning_rate_init_max = 1e-2  #@param {type:\"number\"}\n",
    "learning_rate_init_range = (learning_rate_init_min, learning_rate_init_max)\n",
    "\n",
    "# @markdown Number of trials\n",
    "n_trials = 50  #@param {type:\"slider\", min:10, max:200, step:10}\n",
    "\n",
    "# @markdown Neural network training settings\n",
    "max_iter = 3000  #@param {type:\"number\"}\n",
    "tol = 1e-4  #@param {type:\"number\"}\n",
    "n_iter_no_change = 10  #@param {type:\"number\"}\n",
    "\n",
    "# @markdown Hidden layer adjustments\n",
    "hidden_layer_decrease_factor = 2  #@param {type:\"slider\", min:1, max:4, step:1}\n",
    "min_hidden_layer_size = 8  #@param {type:\"slider\", min:4, max:64, step:1}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4MDxL37ufKuH"
   },
   "outputs": [],
   "source": [
    "#@title Run BoPep!\n",
    "\n",
    "import pyrosetta\n",
    "\n",
    "os.makedirs(\"/content/output\", exist_ok=True)\n",
    "\n",
    "pyrosetta.init('-mute all')\n",
    "\n",
    "from src.run import run_bayesian_optimization\n",
    "\n",
    "run_bayesian_optimization(\n",
    "    embeddings,\n",
    "    target_structure,\n",
    "    target_sequence,\n",
    "\n",
    "    # Docking parameters\n",
    "    num_recycles=1,\n",
    "    num_models=1,\n",
    "    recycle_early_stop_tolerance=0.3,\n",
    "    amber=True,\n",
    "    binding_site_residue_indices=None,\n",
    "    relax_max_iterations=relax_max_iterations,\n",
    "\n",
    "    # BO parameters\n",
    "    iptm_score_weight=iptm_score_weight,\n",
    "    interface_sasa_weight=interface_sasa_weight,\n",
    "    interface_dG_weight=interface_dG_weight,\n",
    "    rosetta_score_weight=rosetta_score_weight,\n",
    "    interface_delta_hbond_unsat_weight=interface_delta_hbond_unsat_weight,\n",
    "    packstat_weight=packstat_weight,\n",
    "    n_initial=n_initial,\n",
    "    n_exploration_iterations=n_exploration_iterations,\n",
    "    n_exploitation_iterations=n_exploitation_iterations,\n",
    "    batch_size=batch_size,\n",
    "    agreeing_models=agreeing_models,\n",
    "    proximity_threshold=proximity_threshold,\n",
    "    hparam_opt_interval=hparam_opt_interval,\n",
    "\n",
    "    # Hyperparameter optimization parameters\n",
    "    n_layers_range=n_layers_range,\n",
    "    n_units_l1_range=n_units_l1_range,\n",
    "    alpha_range=alpha_range,\n",
    "    learning_rate_init_range=learning_rate_init_range,\n",
    "    n_splits=5,\n",
    "    random_state=42,\n",
    "    n_trials=n_trials,\n",
    "    n_jobs=1,\n",
    "    pruner_n_warmup_steps=5,\n",
    "    direction=\"maximize\",\n",
    "    pruner_type=\"MedianPruner\",\n",
    "    sampler_type=\"TPESampler\",\n",
    "    tol=tol,\n",
    "    early_stopping=True,\n",
    "    validation_fraction=0.1,\n",
    "    n_iter_no_change=n_iter_no_change,\n",
    "    hidden_layer_decrease_factor=hidden_layer_decrease_factor,\n",
    "    min_hidden_layer_size=min_hidden_layer_size,\n",
    "    bin_edges=None,\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
